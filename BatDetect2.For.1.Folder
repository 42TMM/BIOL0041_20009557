{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a3bec-f594-4390-a426-11e020681a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import dependencies\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import ipywidgets\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "import xarray as xa\n",
    "from IPython.display import Audio\n",
    "from librosa import display\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "from batdetect2 import api\n",
    "from batdetect2 import plot\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from soundevent import arrays, audio, data\n",
    "from soundevent.data import BoundingBox, Clip, Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0439e22-b21d-4103-b91c-2e721064817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all detections#\n",
    "detections_df = pd.read_parquet(\"kenya_audio_2019/batdetect2/detections/merged.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753b96d-5c4f-46ab-baba-4b8bf6abf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323772a0-71ee-4c8a-b9a6-2258e520bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2b2f3-9d3a-4552-9501-470593f0ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_parquet(\"kenya_audio_2019/batdetect2/features/merged.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0059bd6-2beb-468a-9e9e-fced68db2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e049a-03e3-4047-a677-ca018521a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6803df-a43a-4d46-b6d6-28b6d6a12c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two dataframes together by id column#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5419f6d3-0aec-4831-818d-75cbf223f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call it embeddings_df as analysis is written already for this#\n",
    "embeddings_df = pd.merge(detections_df, embeddings_df, on=\"id\", how=\"outer\")\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e012a-24d1-4932-9068-b64ace9c98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "\n",
    "        #Load functions to add MetaData#\n",
    "\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe6697-8d86-4a1e-b72c-8334f57717b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date from 'filename'\n",
    "def extract_date(path):\n",
    "    # Extract the date (YYYYMMDD)\n",
    "    date_str = path.split('/')[-1].split('_')[1][:8]\n",
    "    # Format the date as YYYY-MM-DD\n",
    "    formatted_date = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    return formatted_date\n",
    "\n",
    "# Extract day from 'filename'#\n",
    "def extract_day(path):\n",
    "    # Extract the date (YYYYMMDD)\n",
    "    date_str = path.split('/')[-1].split('_')[1][:8]\n",
    "    # Extract the day\n",
    "    day = int(date_str[6:8])\n",
    "    return day\n",
    "\n",
    "# Extract the site from the 'filename'#\n",
    "def extract_site(path):\n",
    "    parts = path.split('/')\n",
    "    site_index = parts.index('kenya') + 3  # Find the index of 'kenya' and get the element two positions after\n",
    "    return parts[site_index]\n",
    "\n",
    "# Extract time from 'filename'\n",
    "def extract_time(path):\n",
    "    # Extract the time \n",
    "    time_str = path.split('/')[-1].split('_')[2][:6]\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68b628-3f8b-4a6a-ade9-312173071101",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['site'] = embeddings_df['file_path'].apply(extract_site) \n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f09d5-8911-430d-a7b9-ebbb8981afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['date'] = embeddings_df['file_path'].apply(extract_date) \n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ea24f-d139-4b4c-b692-07be58588da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['time'] = embeddings_df['file_path'].apply(extract_time) \n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff744d-36f5-4b05-80d1-f63bd4b4a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261212d-f241-440a-bcc9-6c79376fbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['duration'] = embeddings_df['end_time']- embeddings_df['start_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37401d1-2442-45ac-acb2-15996487f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['bandwidth'] = embeddings_df['high_freq']-embeddings_df['low_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e6b3d-46b9-4673-ab1d-28c456f8fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#Creating 'gradient' of call\n",
    "####\n",
    "\n",
    "embeddings_df['gradient'] = embeddings_df['bandwidth'] / embeddings_df['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5daf7-ee7a-4fea-8d98-56e46601fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "\n",
    "#Subetting by time#\n",
    "\n",
    "########################\n",
    "\n",
    "# Convert string times to integers for comparison\n",
    "embeddings_df['time'] = embeddings_df['time'].astype(int)\n",
    "\n",
    "# Now create the mask\n",
    "time_mask = (embeddings_df['time'] >= 170000) | (embeddings_df['time'] <= 74500)\n",
    "embeddings_df = embeddings_df[time_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb3907-ba3c-400a-9621-2ca780f166a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "\n",
    "#Subetting by detection score#\n",
    "\n",
    "########################\n",
    "\n",
    "#embeddings_df = embeddings_df[embeddings_df['detection_score'] >= 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ba113-e6bd-4843-a329-f2da46f6aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1117fe5-8b11-435b-a7a3-c591aed7ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "\n",
    "        #Clustering/Dimensionality Reduction#\n",
    "\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb8218-fea4-49dc-9672-4fe198b214aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in embeddings_df.columns if col.startswith(\"feature_\")]\n",
    "features = embeddings_df[feature_columns].values  # Get the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f292525-c666-4550-9bcb-c83e368d4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = PCA(n_components=2).fit_transform(features).T\n",
    "sns.scatterplot(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    hue=embeddings_df[\"species\"],  # Color by species\n",
    "    style=embeddings_df[\"species\"],  # Use different markers by species\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d211f4-7354-4592-a29c-504ede5106ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the variance explained by first two principle components#\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(features)\n",
    "\n",
    "variance_pc1 = pca.explained_variance_ratio_[0]\n",
    "variance_pc2 = pca.explained_variance_ratio_[1]\n",
    "variance_pc3 = pca.explained_variance_ratio_[2]\n",
    "variance_pc4 = pca.explained_variance_ratio_[3]\n",
    "\n",
    "print(f\"Variance explained by PC1: {variance_pc1:.2%}\")\n",
    "print(f\"Variance explained by PC2: {variance_pc2:.2%}\")\n",
    "print(f\"Variance explained by PC3: {variance_pc3:.2%}\")\n",
    "print(f\"Variance explained by PC4: {variance_pc4:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39467750-e33a-4a1a-9e0e-3f4742250403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the variance explained by all principle components#\n",
    "pca_full = PCA(n_components=len(feature_columns))  # Keep all components\n",
    "pca_full.fit(features)\n",
    "\n",
    "print(\"All Principal Components (Eigenvectors):\")\n",
    "print(pca_full.components_)  # Each row corresponds to a principal component\n",
    "\n",
    "# Optional: Plot cumulative explained variance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4998b9-1d35-435f-8446-f301e852424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------#\n",
    "        #COMPARING 2 PCs vs 32 PCs #\n",
    "#-------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5dc3c-c137-40c9-a96a-97c26ac1f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means clustering for functional group analysis#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ff27d-443f-409c-b045-b32a7d0d19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef830ff1-2c1c-48ad-97c9-63c1311dac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(X, Y))\n",
    "inertias = []\n",
    "\n",
    "for i in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,15), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c45d3a-e016-4f3b-9ed2-b7d8fb8b8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with 32 components\n",
    "pca = PCA(n_components=32)\n",
    "pc_transformed = pca.fit_transform(features)  # Get transformed PCA features\n",
    "\n",
    "# Convert to DataFrame for easy handling\n",
    "pca_df_2 = embeddings_df.copy()\n",
    "pca_df_2[[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\",\"PC14\",\"PC15\",\"PC16\",\"PC17\", \"PC18\", \"PC19\", \"PC20\", \"PC21\",\"PC22\",\"PC23\",\"PC24\",\"PC25\", \"PC26\", \"PC27\", \"PC28\", \"PC29\",\"PC30\",\"PC31\",\"PC32\"]] = pc_transformed  # Add PCs to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2f9a9-3e6d-4a8b-8c4b-ec6efab39253",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_columns = [col for col in pca_df_2.columns if col.startswith(\"PC\")]\n",
    "\n",
    "features_for_clustering = pc_columns + ['high_freq', 'low_freq', 'duration']\n",
    "\n",
    "# Apply K-Means to all PCs\n",
    "k = 7  # Adjust the number of clusters if needed\n",
    "kmeans_maxPCs = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "pca_df_2[\"Cluster_32PCs\"] = kmeans_maxPCs.fit_predict(pca_df_2[features_for_clustering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd595b-1701-4109-9f84-b7b71759520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8)) \n",
    "\n",
    "sns.scatterplot(data=pca_df_2, \n",
    "                x=\"PC1\", \n",
    "                y=\"PC2\", \n",
    "                hue=\"Cluster_32PCs\", \n",
    "                palette=\"deep\",  \n",
    "                legend=\"full\")  \n",
    "\n",
    "plt.title(\"K-Means Clusters in PC1 vs PC2 Space\")  \n",
    "plt.xlabel(\"First Principal Component (PC1)\")  \n",
    "plt.ylabel(\"Second Principal Component (PC2)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23289e73-9514-4633-b856-231ea7ca6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d810a-40d5-4b45-b35e-f23ed1fb31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1a799-193b-4c4c-aa7d-f88e361e88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, min_samples=10)  # You can adjust these parameters\n",
    "pca_df_2[\"Cluster_HDBSCAN\"] = clusterer.fit_predict(pca_df_2[features_for_clustering])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d71eb3-9a88-41d8-9663-1ac80e19951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=pca_df_2, \n",
    "                x='PC1', \n",
    "                y='PC2',\n",
    "                hue='Cluster_HDBSCAN',\n",
    "                palette='deep',\n",
    "                legend='full')\n",
    "\n",
    "plt.title('HDBSCAN Clusters in PC1 vs PC2 Space')\n",
    "plt.xlabel('First Principal Component (PC1)')\n",
    "plt.ylabel('Second Principal Component (PC2)')\n",
    "\n",
    "# Add a note about noise points (cluster -1) if they exist\n",
    "if -1 in pca_df_2['Cluster_HDBSCAN'].unique():\n",
    "    plt.text(0.02, 0.98, 'Cluster -1 represents noise points', \n",
    "             transform=plt.gca().transAxes, \n",
    "             fontsize=9, \n",
    "             alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4205bbf-ccc0-42c5-a497-1f7628850b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#GMM\n",
    "gmm = GaussianMixture(n_components=7, random_state=42)\n",
    "pca_df_2[\"Cluster_GMM\"] = gmm.fit_predict(pca_df_2[features_for_clustering])\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=pca_df_2, \n",
    "                x='PC1', \n",
    "                y='PC2',\n",
    "                hue='Cluster_GMM',\n",
    "                palette='deep',\n",
    "                legend='full')\n",
    "\n",
    "plt.title('GMM Clusters in PC1 vs PC2 Space')\n",
    "plt.xlabel('First Principal Component (PC1)')\n",
    "plt.ylabel('Second Principal Component (PC2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deaf4a0-d3bd-4478-807d-f85ae1ceaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the clustering so I can change it from K-Means to some other clustering methods#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079fdde-6b04-4cde-a4d1-8011254e7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_avg = silhouette_score(pca_df_2[features_for_clustering], pca_df_2[\"Cluster_GMM\"])\n",
    "print(f\"The average silhouette score is: {silhouette_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a4894-9e9c-43a1-bbf6-eb8d0ca8e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "davies_bouldin_avg = davies_bouldin_score(pca_df_2[features_for_clustering], pca_df_2[\"Cluster_GMM\"])\n",
    "print(f\"The Davies-Bouldin index is: {davies_bouldin_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fd38f-b439-47bc-8079-699c76b9bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8bbfc-757c-4f50-b326-82b2ce75fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "\n",
    "        #Functional Group Analysis#\n",
    "\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ab85a-38cf-4d67-88f9-eb2a9b4a5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "def extract_detection_section(\n",
    "    path: str,\n",
    "    start_time: float,\n",
    "    end_time: float,\n",
    "    low_freq: float,\n",
    "    high_freq: float,\n",
    "    time_buffer: float = 0.005,\n",
    "    freq_buffer: float = 2000,\n",
    "    window_size: float = 0.002,\n",
    "    window_overlap: float = 0.75,\n",
    "):\n",
    "    recording = Recording.from_file(path, compute_hash=False)\n",
    "    wave = audio.load_clip(\n",
    "        Clip(\n",
    "            recording=recording,\n",
    "            start_time=max(start_time - 2 * time_buffer, 0),\n",
    "            end_time=min(end_time + 2 * time_buffer, recording.duration),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    spectrogram = audio.compute_spectrogram(\n",
    "        wave,\n",
    "        window_size=window_size,\n",
    "        hop_size=window_size * (1 - window_overlap),\n",
    "    )\n",
    "\n",
    "    db_spectrogram = arrays.to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    return db_spectrogram.sel(\n",
    "        time=slice(start_time - time_buffer, end_time + time_buffer),\n",
    "        frequency=slice(low_freq - freq_buffer, high_freq + freq_buffer),\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60710d-84a0-4c52-8313-329ce25ba489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_files_column(pca_df_2):\n",
    "    # Define the old and new prefixes\n",
    "    old_prefix = \"downloads/biome_health_project_files/country_files/kenya/working_data/\"\n",
    "    new_prefix = \"kenya_audio_2019/audio/kenya/working_data/\"\n",
    "\n",
    "    # Get all valid files from the working_data directory\n",
    "    folder_path = \"/home/jupyter-tom/kenya_audio_2019/audio/kenya/working_data\"\n",
    "    working_data_files = set()\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".WAV\"):  # Adjust if needed\n",
    "                relative_path = os.path.relpath(os.path.join(root, file_name), folder_path)\n",
    "                working_data_files.add(relative_path)\n",
    "\n",
    "    # Function to match and replace file paths\n",
    "    def match_file(file_path):\n",
    "        if file_path.startswith(old_prefix):\n",
    "            relative_path = file_path[len(old_prefix):]  # Extract relative path\n",
    "            if relative_path in working_data_files:\n",
    "                return os.path.join(new_prefix, relative_path)\n",
    "        return None\n",
    "\n",
    "    # Apply function to create the 'files' column\n",
    "    pca_df_2[\"files\"] = pca_df_2[\"file_path\"].apply(match_file)\n",
    "\n",
    "    return pca_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bc776-1ca4-409f-a286-a84c1332d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2 = add_files_column(pca_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f293c4f-c89e-4d1e-ab06-dafa608d9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_files_column(pca_df_2):\n",
    "    # Define the old and new prefixes\n",
    "    old_prefix = \"downloads/biome_health_project_files/country_files/kenya/working_data/\"\n",
    "    new_prefix = \"kenya_audio_2019/audio/cluster_samples/audio/kenya/working_data/\"\n",
    "\n",
    "    # Get all valid files from the cluster_samples directory\n",
    "    folder_path = \"/home/jupyter-tom/kenya_audio_2019/audio/cluster_samples/audio/kenya/working_data/\"\n",
    "    cluster_sample_files = set()\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".WAV\"):  # Adjust if needed\n",
    "                relative_path = os.path.relpath(os.path.join(root, file_name), folder_path)\n",
    "                cluster_sample_files.add(relative_path)\n",
    "\n",
    "    # Function to match and replace file paths\n",
    "    def match_file(file_path):\n",
    "        if file_path.startswith(old_prefix):\n",
    "            relative_path = file_path[len(old_prefix):]  # Extract relative path\n",
    "            if relative_path in cluster_sample_files:\n",
    "                return os.path.join(new_prefix, relative_path)\n",
    "        return None\n",
    "\n",
    "    # Apply function to create the 'files' column\n",
    "    pca_df_2[\"files\"] = pca_df_2[\"file_path\"].apply(match_file)\n",
    "\n",
    "    return pca_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04358a41-1814-40b2-b013-3b0ba7eedcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2 = add_files_column(pca_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ac81c-1b1b-46cc-a55b-bb302b11cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#Plotting clusters with representative spectrograms\n",
    "#WITH ZOOMED IN DATAPOINTS- \n",
    "#####################################################\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec, lines\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def border_size(rows, cols):\n",
    "    \"\"\"Returns the number of border cells in a grid of given rows and columns.\"\"\"\n",
    "    return 2 * (rows + cols - 2)  # Sum of all outer cells minus double-counted corners\n",
    "\n",
    "# Set the style of the visualisation\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Define a colour-blind friendly palette with three colors\n",
    "colorblind_palette = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#CC79A7\", \"#0072B2\", \"#D55E00\"]\n",
    "\n",
    "def scatter_spec(\n",
    "    pca_df_2,\n",
    "    column_size=8,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    groupby=\"Cluster_32PCs\",\n",
    "    pal_color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#CC79A7\", \"#0072B2\", \"#D55E00\"],\n",
    "    matshow_kwargs={\"cmap\": \"viridis\"},\n",
    "    scatter_kwargs={\"alpha\": 0.75, \"s\": 40},\n",
    "    line_kwargs={\"lw\": 1, \"ls\": \"dashed\", \"alpha\": 0.5, \"color\": \"black\"},  # Changed default line color to black\n",
    "    figsize=(20, 20),\n",
    "    dpi=900,\n",
    "    range_pad=0.1,\n",
    "    x_range=None,\n",
    "    y_range=None,\n",
    "    draw_lines=True,\n",
    "):\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(column_size, column_size)\n",
    "\n",
    "    # Determine x range using PC values\n",
    "    if x_range is None:\n",
    "        xmin = pca_df_2[x].min()\n",
    "        xmax = pca_df_2[x].max()\n",
    "        xmin -= (xmax - xmin) * range_pad\n",
    "        xmax += (xmax - xmin) * range_pad\n",
    "    else:\n",
    "        xmin, xmax = x_range\n",
    "\n",
    "    # Determine y range using PC values\n",
    "    if y_range is None:\n",
    "        ymin = pca_df_2[y].min()\n",
    "        ymax = pca_df_2[y].max()\n",
    "        ymin -= (ymax - ymin) * range_pad\n",
    "        ymax += (ymax - ymin) * range_pad\n",
    "    else:\n",
    "        ymin, ymax = y_range\n",
    "\n",
    "    # Main scatter plot\n",
    "    main_ax = fig.add_subplot(gs[1:column_size-1, 1:column_size-1])\n",
    "    \n",
    "    # Plot scatter points\n",
    "    sns.scatterplot(\n",
    "        data=pca_df_2,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=groupby,\n",
    "        palette=pal_color,\n",
    "        edgecolor=\"none\",\n",
    "        ax=main_ax,\n",
    "        legend=True,\n",
    "        **scatter_kwargs,\n",
    "    )\n",
    "\n",
    "    unique_labels = list(pca_df_2[groupby].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Configure main plot\n",
    "    main_ax.set_xlabel(x, fontsize=16, labelpad=15)\n",
    "    main_ax.set_ylabel(y, fontsize=16, labelpad=15)\n",
    "    main_ax.set_xlim((xmin, xmax))\n",
    "    main_ax.set_ylim((ymin, ymax))\n",
    "    main_ax.tick_params(axis=\"both\", labelsize=20)\n",
    "    main_ax.spines[\"top\"].set_visible(False)\n",
    "    main_ax.spines[\"right\"].set_visible(False)\n",
    "    main_ax.spines[\"left\"].set_position((\"axes\", 0.04))\n",
    "    main_ax.spines[\"bottom\"].set_position((\"axes\", 0.04))\n",
    "    main_ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        direction=\"in\",\n",
    "        length=6,\n",
    "        width=1,\n",
    "        pad=2,\n",
    "        labelsize=12,\n",
    "    )\n",
    "\n",
    "    # Drop NaN values from the 'files' column\n",
    "    pca_df_2_clean = pca_df_2.dropna(subset=['files'])\n",
    "\n",
    "    # Select one representative sample per cluster\n",
    "    subset = (\n",
    "        pca_df_2_clean.groupby(groupby)\n",
    "        .sample(n=1, random_state=10)\n",
    "        .reset_index()\n",
    "        .sort_values(groupby, key=lambda x: x.map(unique_labels.index))\n",
    "    )\n",
    "\n",
    "    # Calculate evenly distributed positions for spectrograms\n",
    "    positions = []\n",
    "\n",
    "    # Calculate number of spectrograms for each edge\n",
    "    total_positions = column_size * 4 - 4  # Total available positions on the border\n",
    "    specs_per_edge = max(1, n_clusters // 4)  # Distribute clusters evenly across edges\n",
    "\n",
    "    # Top edge positions (excluding corners)\n",
    "    step = (column_size - 2) // (specs_per_edge + 1)\n",
    "    for i in range(1, min(specs_per_edge + 1, n_clusters + 1)):\n",
    "        positions.append((0, i * step))\n",
    "\n",
    "    # Right edge positions (excluding corners)\n",
    "    if n_clusters > specs_per_edge:\n",
    "        remaining = n_clusters - len(positions)\n",
    "        step = (column_size - 2) // (specs_per_edge + 1)\n",
    "        for i in range(1, min(specs_per_edge + 1, remaining + 1)):\n",
    "            positions.append((i * step, column_size - 1))\n",
    "\n",
    "    # Bottom edge positions (excluding corners)\n",
    "    if n_clusters > 2 * specs_per_edge:\n",
    "        remaining = n_clusters - len(positions)\n",
    "        step = (column_size - 2) // (specs_per_edge + 1)\n",
    "        for i in range(1, min(specs_per_edge + 1, remaining + 1)):\n",
    "            positions.append((column_size - 1, column_size - 1 - i * step))\n",
    "\n",
    "    # Left edge positions (excluding corners)\n",
    "    if n_clusters > 3 * specs_per_edge:\n",
    "        remaining = n_clusters - len(positions)\n",
    "        step = (column_size - 2) // (specs_per_edge + 1)\n",
    "        for i in range(1, min(specs_per_edge + 1, remaining + 1)):\n",
    "            positions.append((column_size - 1 - i * step, 0))\n",
    "\n",
    "    # Add any remaining positions to corners if needed\n",
    "    if len(positions) < n_clusters:\n",
    "        corners = [(0, 0), (0, column_size-1), (column_size-1, column_size-1), (column_size-1, 0)]\n",
    "        for corner in corners:\n",
    "            if len(positions) < n_clusters:\n",
    "                positions.append(corner)\n",
    "\n",
    "    col_num = subset.columns.get_loc(groupby)\n",
    "\n",
    "    for idx, (pca_df_2_row, position) in enumerate(zip(subset.itertuples(), positions)):\n",
    "        row, col = position\n",
    "\n",
    "        # Create the subplot first\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "        # Extract the file path\n",
    "        file_path = pca_df_2_row.files\n",
    "\n",
    "        # Check if the file path is valid\n",
    "        if pd.isna(file_path) or file_path == \"\":\n",
    "            continue  # Skip invalid paths\n",
    "\n",
    "        # Extract and plot the spectrogram\n",
    "        spec = extract_detection_section(pca_df_2_row.files,\n",
    "                                         pca_df_2_row.start_time,\n",
    "                                         pca_df_2_row.end_time,\n",
    "                                         pca_df_2_row.low_freq,\n",
    "                                         pca_df_2_row.high_freq)\n",
    "\n",
    "\n",
    "        # Define time and frequency limits\n",
    "        min_time, max_time = spec.time.values[[0, -1]]\n",
    "        min_freq, max_freq = spec.frequency.values[[0, -1]]\n",
    "\n",
    "        # Plot with correctly mapped axes\n",
    "        ax.imshow(spec, aspect='auto', cmap='magma', origin=\"lower\",\n",
    "                  extent=[min_time, max_time, min_freq, max_freq])\n",
    "\n",
    "        # Set title with cluster information\n",
    "        ax.set_title(f\"Cluster {pca_df_2_row.Cluster_32PCs}\")\n",
    "\n",
    "        if draw_lines:\n",
    "            # Draw connecting lines\n",
    "            mytrans = ax.transAxes + ax.figure.transFigure.inverted()\n",
    "            line_end_pos = mytrans.transform((0.5, 0.5))\n",
    "\n",
    "            xpos, ypos = main_ax.transLimits.transform(\n",
    "                (getattr(pca_df_2_row, x), getattr(pca_df_2_row, y))\n",
    "            )\n",
    "            mytrans2 = main_ax.transAxes + main_ax.figure.transFigure.inverted()\n",
    "            infig_position_start = mytrans2.transform([xpos, ypos])\n",
    "\n",
    "            # Use black lines instead of cluster colors\n",
    "            fig.lines.append(\n",
    "                lines.Line2D(\n",
    "                    [infig_position_start[0], line_end_pos[0]],\n",
    "                    [infig_position_start[1], line_end_pos[1]],\n",
    "                    transform=fig.transFigure,\n",
    "                    **line_kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203551da-ff97-4d2a-847a-75b68a6c1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting spectograms on the same scale#\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec, lines\n",
    "from scipy.ndimage import zoom\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def extract_standardized_spectrogram(\n",
    "    path: str,\n",
    "    start_time: float,\n",
    "    end_time: float,\n",
    "    window_size: float = 0.002,\n",
    "    window_overlap: float = 0.75,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract a spectrogram with standardized 1-second window centered on the midpoint \n",
    "    of detection and full frequency range (0-120,000 Hz).\n",
    "    \"\"\"\n",
    "    recording = Recording.from_file(path, compute_hash=False)\n",
    "    \n",
    "    # Calculate the midpoint of the detection\n",
    "    midpoint = (start_time + end_time) / 2\n",
    "    \n",
    "    # Create a 1-second window centered on the midpoint\n",
    "    new_start = max(midpoint - 0.025, 0)\n",
    "    new_end = min(midpoint + 0.025, recording.duration)\n",
    "    \n",
    "    # Load audio clip\n",
    "    wave = audio.load_clip(\n",
    "        Clip(\n",
    "            recording=recording,\n",
    "            start_time=new_start,\n",
    "            end_time=new_end,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    spectrogram = audio.compute_spectrogram(\n",
    "        wave,\n",
    "        window_size=window_size,\n",
    "        hop_size=window_size * (1 - window_overlap),\n",
    "    )\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    db_spectrogram = arrays.to_db(spectrogram, ref=np.max)\n",
    "    \n",
    "    # Return the full spectrogram with standardized frequency range\n",
    "    return db_spectrogram.sel(\n",
    "        frequency=slice(10000, 80000)\n",
    "    )\n",
    "\n",
    "def border_size(rows, cols):\n",
    "    \"\"\"Returns the number of border cells in a grid of given rows and columns.\"\"\"\n",
    "    return 2 * (rows + cols - 2)  # Sum of all outer cells minus double-counted corners\n",
    "\n",
    "# Set the style of the visualisation\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Define a colour-blind friendly palette with seven colors\n",
    "colorblind_palette = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#CC79A7\", \"#0072B2\", \"#D55E00\"]\n",
    "\n",
    "def scatter_spec(\n",
    "    pca_df_2,\n",
    "    column_size=8,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    groupby=\"Cluster_32PCs\",\n",
    "    pal_color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#CC79A7\", \"#0072B2\", \"#D55E00\"],\n",
    "    matshow_kwargs={\"cmap\": \"viridis\"},\n",
    "    scatter_kwargs={\"alpha\": 0.75, \"s\": 40},\n",
    "    line_kwargs={\"lw\": 1, \"ls\": \"dashed\", \"alpha\": 0.5, \"color\": \"black\"},\n",
    "    figsize=(20, 20),\n",
    "    dpi=900,\n",
    "    range_pad=0.1,\n",
    "    x_range=None,\n",
    "    y_range=None,\n",
    "    draw_lines=True,\n",
    "):\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(column_size, column_size)\n",
    "\n",
    "    # Determine x range using PC values\n",
    "    if x_range is None:\n",
    "        xmin = pca_df_2[x].min()\n",
    "        xmax = pca_df_2[x].max()\n",
    "        xmin -= (xmax - xmin) * range_pad\n",
    "        xmax += (xmax - xmin) * range_pad\n",
    "    else:\n",
    "        xmin, xmax = x_range\n",
    "\n",
    "    # Determine y range using PC values\n",
    "    if y_range is None:\n",
    "        ymin = pca_df_2[y].min()\n",
    "        ymax = pca_df_2[y].max()\n",
    "        ymin -= (ymax - ymin) * range_pad\n",
    "        ymax += (ymax - ymin) * range_pad\n",
    "    else:\n",
    "        ymin, ymax = y_range\n",
    "\n",
    "    # Main scatter plot\n",
    "    main_ax = fig.add_subplot(gs[1:column_size-1, 1:column_size-1])\n",
    "    \n",
    "    # Plot scatter points\n",
    "    sns.scatterplot(\n",
    "        data=pca_df_2,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=groupby,\n",
    "        palette=pal_color,\n",
    "        edgecolor=\"none\",\n",
    "        ax=main_ax,\n",
    "        legend=True,\n",
    "        **scatter_kwargs,\n",
    "    )\n",
    "\n",
    "    unique_labels = list(pca_df_2[groupby].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Configure main plot\n",
    "    main_ax.set_xlabel(x, fontsize=16, labelpad=15)\n",
    "    main_ax.set_ylabel(y, fontsize=16, labelpad=15)\n",
    "    main_ax.set_xlim((xmin, xmax))\n",
    "    main_ax.set_ylim((ymin, ymax))\n",
    "    main_ax.tick_params(axis=\"both\", labelsize=20)\n",
    "    main_ax.spines[\"top\"].set_visible(False)\n",
    "    main_ax.spines[\"right\"].set_visible(False)\n",
    "    main_ax.spines[\"left\"].set_position((\"axes\", 0.04))\n",
    "    main_ax.spines[\"bottom\"].set_position((\"axes\", 0.04))\n",
    "    main_ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        direction=\"in\",\n",
    "        length=6,\n",
    "        width=1,\n",
    "        pad=2,\n",
    "        labelsize=12,\n",
    "    )\n",
    "\n",
    "    # Drop NaN values from the 'files' column\n",
    "    pca_df_2_clean = pca_df_2.dropna(subset=['files'])\n",
    "\n",
    "    # Select one representative sample per cluster\n",
    "    subset = (\n",
    "        pca_df_2_clean.groupby(groupby)\n",
    "        .sample(n=1, random_state=45)\n",
    "        .reset_index()\n",
    "        .sort_values(groupby, key=lambda x: x.map(unique_labels.index))\n",
    "    )\n",
    "\n",
    "    # Calculate evenly distributed positions for spectrograms\n",
    "    positions = []\n",
    "\n",
    "    # Calculate number of spectrograms for each edge\n",
    "    total_positions = column_size * 4 - 4  # Total available positions on the border\n",
    "    specs_per_edge = max(1, n_clusters // 4)  # Distribute clusters evenly across edges\n",
    "\n",
    "    # Top edge positions (excluding corners)\n",
    "    step = (column_size - 2) // (specs_per_edge + 1)\n",
    "    for i in range(1, min(specs_per_edge + 1, n_clusters + 1)):\n",
    "        positions.append((0, i * step))\n",
    "\n",
    "    # Right edge positions (excluding corners)\n",
    "    if n_clusters > specs_per_edge:\n",
    "        remaining = n_clusters - len(positions)\n",
    "        step = (column_size - 2) // (specs_per_edge + 1)\n",
    "        for i in range(1, min(specs_per_edge + 1, remaining + 1)):\n",
    "            positions.append((i * step, column_size - 1))\n",
    "\n",
    "    # Bottom edge positions (excluding corners)\n",
    "    if n_clusters > 2 * specs_per_edge:\n",
    "        remaining = n_clusters - len(positions)\n",
    "        step = (column_size - 2) // (specs_per_edge + 1)\n",
    "        for i in range(1, min(specs_per_edge + 1, remaining + 1)):\n",
    "            positions.append((column_size - 1, column_size - 1 - i * step))\n",
    "\n",
    "    # Left edge positions (excluding corners)\n",
    "    if n_clusters > 3 * specs_per_edge:\n",
    "        remaining = n_clusters - len(positions)\n",
    "        step = (column_size - 2) // (specs_per_edge + 1)\n",
    "        for i in range(1, min(specs_per_edge + 1, remaining + 1)):\n",
    "            positions.append((column_size - 1 - i * step, 0))\n",
    "\n",
    "    # Add any remaining positions to corners if needed\n",
    "    if len(positions) < n_clusters:\n",
    "        corners = [(0, 0), (0, column_size-1), (column_size-1, column_size-1), (column_size-1, 0)]\n",
    "        for corner in corners:\n",
    "            if len(positions) < n_clusters:\n",
    "                positions.append(corner)\n",
    "\n",
    "    for idx, (pca_df_2_row, position) in enumerate(zip(subset.itertuples(), positions)):\n",
    "        row, col = position\n",
    "\n",
    "        # Create the subplot first\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "        # Extract the file path\n",
    "        file_path = pca_df_2_row.files\n",
    "\n",
    "        # Check if the file path is valid\n",
    "        if pd.isna(file_path) or file_path == \"\":\n",
    "            continue  # Skip invalid paths\n",
    "\n",
    "        # Extract and plot the standardized spectrogram with 1-second window and full frequency range\n",
    "        spec = extract_standardized_spectrogram(\n",
    "            pca_df_2_row.files,\n",
    "            pca_df_2_row.start_time,\n",
    "            pca_df_2_row.end_time\n",
    "        )\n",
    "\n",
    "        # Define time and frequency limits\n",
    "        min_time, max_time = spec.time.values[[0, -1]]\n",
    "        min_freq, max_freq = spec.frequency.values[[0, -1]]  # Should be 0-120000 Hz\n",
    "\n",
    "        # Plot with correctly mapped axes\n",
    "        im = ax.imshow(spec, aspect='auto', cmap='magma', origin=\"lower\",\n",
    "                  extent=[min_time, max_time, min_freq, max_freq])\n",
    "        \n",
    "        # Add a title that shows the cluster\n",
    "        ax.set_title(f\"Cluster {getattr(pca_df_2_row, groupby)}\", fontsize=10)\n",
    "        \n",
    "        # Add standardized labels for time and frequency\n",
    "        ax.set_xlabel(\"Time (s)\", fontsize=8)\n",
    "        ax.set_ylabel(\"Frequency (Hz)\", fontsize=8)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "\n",
    "        if draw_lines:\n",
    "            # Draw connecting lines\n",
    "            mytrans = ax.transAxes + ax.figure.transFigure.inverted()\n",
    "            line_end_pos = mytrans.transform((0.5, 0.5))\n",
    "\n",
    "            xpos, ypos = main_ax.transLimits.transform(\n",
    "                (getattr(pca_df_2_row, x), getattr(pca_df_2_row, y))\n",
    "            )\n",
    "            mytrans2 = main_ax.transAxes + main_ax.figure.transFigure.inverted()\n",
    "            infig_position_start = mytrans2.transform([xpos, ypos])\n",
    "\n",
    "            # Use black lines instead of cluster colors\n",
    "            fig.lines.append(\n",
    "                lines.Line2D(\n",
    "                    [infig_position_start[0], line_end_pos[0]],\n",
    "                    [infig_position_start[1], line_end_pos[1]],\n",
    "                    transform=fig.transFigure,\n",
    "                    **line_kwargs,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d3659-732c-4413-8c37-8ff6d12f7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#without lines \n",
    "#########\n",
    "\n",
    "def scatter_spec(\n",
    "    pca_df_2,\n",
    "    column_size=8,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    groupby=\"Cluster_32PCs\",\n",
    "    pal_color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#CC79A7\", \"#0072B2\", \"#D55E00\"],\n",
    "    matshow_kwargs={\"cmap\": \"viridis\"},\n",
    "    scatter_kwargs={\"alpha\": 0.75, \"s\": 40},\n",
    "    figsize=(20, 20),\n",
    "    dpi=900,\n",
    "    range_pad=0.1,\n",
    "    x_range=None,\n",
    "    y_range=None,\n",
    "    samples_per_cluster=4,  # Number of spectrograms to show per cluster\n",
    "    main_plot_size_ratio=0.7,  # Controls the size of the main plot (smaller values = smaller plot)\n",
    "    random_state=45  # Customizable random state for reproducibility\n",
    "):\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(column_size, column_size)\n",
    "\n",
    "    # Determine x range using PC values\n",
    "    if x_range is None:\n",
    "        xmin = pca_df_2[x].min()\n",
    "        xmax = pca_df_2[x].max()\n",
    "        xmin -= (xmax - xmin) * range_pad\n",
    "        xmax += (xmax - xmin) * range_pad\n",
    "    else:\n",
    "        xmin, xmax = x_range\n",
    "\n",
    "    # Determine y range using PC values\n",
    "    if y_range is None:\n",
    "        ymin = pca_df_2[y].min()\n",
    "        ymax = pca_df_2[y].max()\n",
    "        ymin -= (ymax - ymin) * range_pad\n",
    "        ymax += (ymax - ymin) * range_pad\n",
    "    else:\n",
    "        ymin, ymax = y_range\n",
    "\n",
    "    # Calculate the center indices based on column_size\n",
    "    center_start = int(column_size * (1-main_plot_size_ratio)/2)\n",
    "    center_end = int(column_size * (1+main_plot_size_ratio)/2)\n",
    "    \n",
    "    # Main scatter plot - smaller size (centered)\n",
    "    main_ax = fig.add_subplot(gs[center_start:center_end, center_start:center_end])\n",
    "    \n",
    "    # Plot scatter points\n",
    "    sns.scatterplot(\n",
    "        data=pca_df_2,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=groupby,\n",
    "        palette=pal_color,\n",
    "        edgecolor=\"none\",\n",
    "        ax=main_ax,\n",
    "        legend=True,\n",
    "        **scatter_kwargs,\n",
    "    )\n",
    "\n",
    "    unique_labels = sorted(pca_df_2[groupby].unique())\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Configure main plot\n",
    "    main_ax.set_xlabel(x, fontsize=16, labelpad=15)\n",
    "    main_ax.set_ylabel(y, fontsize=16, labelpad=15)\n",
    "    main_ax.set_xlim((xmin, xmax))\n",
    "    main_ax.set_ylim((ymin, ymax))\n",
    "    main_ax.tick_params(axis=\"both\", labelsize=20)\n",
    "    main_ax.spines[\"top\"].set_visible(False)\n",
    "    main_ax.spines[\"right\"].set_visible(False)\n",
    "    main_ax.spines[\"left\"].set_position((\"axes\", 0.04))\n",
    "    main_ax.spines[\"bottom\"].set_position((\"axes\", 0.04))\n",
    "    main_ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        direction=\"in\",\n",
    "        length=6,\n",
    "        width=1,\n",
    "        pad=2,\n",
    "        labelsize=12,\n",
    "    )\n",
    "\n",
    "    # Drop NaN values from the 'files' column\n",
    "    pca_df_2_clean = pca_df_2.dropna(subset=['files'])\n",
    "\n",
    "    # Select multiple samples per cluster with customizable random_state\n",
    "    all_samples = []\n",
    "    for cluster in unique_labels:\n",
    "        cluster_samples = pca_df_2_clean[pca_df_2_clean[groupby] == cluster]\n",
    "        if len(cluster_samples) >= samples_per_cluster:\n",
    "            # Get samples_per_cluster samples\n",
    "            selected_samples = cluster_samples.sample(n=samples_per_cluster, random_state=random_state)\n",
    "        else:\n",
    "            # If we don't have enough samples, just use what we have with replacement\n",
    "            selected_samples = cluster_samples.sample(n=samples_per_cluster, replace=True, random_state=random_state)\n",
    "        all_samples.append(selected_samples)\n",
    "    \n",
    "    subset = pd.concat(all_samples).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate total number of spectrograms\n",
    "    total_specs = n_clusters * samples_per_cluster\n",
    "    \n",
    "    # Generate evenly distributed positions around the entire border\n",
    "    def generate_even_positions_around_border(column_size, total_positions):\n",
    "        \"\"\"Generate positions evenly spaced around the entire border of the grid.\"\"\"\n",
    "        positions = []\n",
    "        perimeter = 2 * (column_size-1) + 2 * (column_size-1)  # Total cells on perimeter\n",
    "        \n",
    "        if total_positions > perimeter:\n",
    "            # If we need more positions than perimeter cells, we'll have to double up\n",
    "            print(f\"Warning: Requested {total_positions} positions but only {perimeter} available on perimeter.\")\n",
    "            # Will place positions as evenly as possible\n",
    "        \n",
    "        # Calculate spacing - how many cells to skip between positions\n",
    "        spacing = max(1, perimeter / total_positions)\n",
    "        \n",
    "        # Start generating positions, walking around the border\n",
    "        current_pos = 0\n",
    "        for i in range(total_positions):\n",
    "            # Calculate the position index along the perimeter\n",
    "            pos_index = int(current_pos) % perimeter\n",
    "            \n",
    "            # Convert perimeter index to grid coordinates\n",
    "            if pos_index < column_size-1:  # Top edge\n",
    "                positions.append((0, pos_index+1))  # +1 to avoid the corner\n",
    "            elif pos_index < 2*(column_size-1):  # Right edge\n",
    "                positions.append((pos_index-(column_size-1)+1, column_size-1))\n",
    "            elif pos_index < 3*(column_size-1):  # Bottom edge\n",
    "                positions.append((column_size-1, column_size-1-(pos_index-2*(column_size-1))-1))\n",
    "            else:  # Left edge\n",
    "                positions.append((column_size-1-(pos_index-3*(column_size-1))-1, 0))\n",
    "            \n",
    "            # Increment for next position\n",
    "            current_pos += spacing\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    # Get evenly distributed positions around the border\n",
    "    positions = generate_even_positions_around_border(column_size, total_specs)\n",
    "    \n",
    "    # Loop through each sample and create a spectrogram\n",
    "    for i, (idx, row) in enumerate(subset.iterrows()):\n",
    "        if i >= len(positions):\n",
    "            break  # Safety check\n",
    "            \n",
    "        cluster_label = row[groupby]\n",
    "        grid_row, grid_col = positions[i]\n",
    "        \n",
    "        # Skip positions that would overlap with the main plot\n",
    "        if (center_start <= grid_row < center_end) and (center_start <= grid_col < center_end):\n",
    "            continue\n",
    "        \n",
    "        # Create subplot for spectrogram\n",
    "        ax = fig.add_subplot(gs[grid_row, grid_col])\n",
    "    \n",
    "        label_index = unique_labels.index(cluster_label)\n",
    "        cluster_color = pal_color[label_index % len(pal_color)]\n",
    "    \n",
    "        rect = Rectangle(\n",
    "            (-0.05, -0.05), 1.5, 1.5,  # slightly larger than (0, 0, 1, 1)\n",
    "            transform=ax.transAxes,\n",
    "            color=cluster_color,\n",
    "            alpha=0.7,\n",
    "            zorder=0\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "        # Extract the file path\n",
    "        file_path = row['files']    ### <----- Here\n",
    "        \n",
    "        # Check if the file path is valid\n",
    "        if pd.isna(file_path) or file_path == \"\":\n",
    "            continue  # Skip invalid paths\n",
    "\n",
    "        try:\n",
    "            # Extract and plot the standardized spectrogram\n",
    "            spec = extract_standardized_spectrogram(\n",
    "                file_path,\n",
    "                row['start_time'],\n",
    "                row['end_time']\n",
    "            )\n",
    "\n",
    "            # Define time and frequency limits\n",
    "            min_time, max_time = spec.time.values[[0, -1]]\n",
    "            min_freq, max_freq = spec.frequency.values[[0, -1]]\n",
    "\n",
    "            # Plot spectrogram\n",
    "            im = ax.imshow(spec, aspect='auto', cmap='magma', origin=\"lower\",\n",
    "                      extent=[min_time, max_time, min_freq, max_freq])\n",
    "            \n",
    "            # Add a title that shows the cluster\n",
    "            ax.set_title(f\"Cluster {cluster_label}\", fontsize=10)\n",
    "            \n",
    "            # Add standardized labels\n",
    "            if grid_col == 0:  # Only leftmost subplots\n",
    "                ax.set_ylabel(\"Frequency (kHz)\", fontsize=8)\n",
    "                ax.yaxis.set_major_formatter(FuncFormatter(lambda val, pos: f\"{val/1000:.1f}\"))\n",
    "            else:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.yaxis.set_ticks([])\n",
    "            ax.set_xlabel(\"\", fontsize=8)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing spectrogram for cluster {cluster_label}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Execute tight_layout to ensure all axes positions are finalized\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b6d57-9b46-4436-86ff-41f0ac5adf07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter_spec(pca_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99128e-65d9-46d6-95b4-f044b71553f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detection_section(\n",
    "    path: str,\n",
    "    start_time: float,\n",
    "    end_time: float,\n",
    "    low_freq: float,\n",
    "    high_freq: float,\n",
    "    time_buffer: float = 0.02,\n",
    "    freq_buffer: float = 2000,\n",
    "    window_size: float = 0.002,\n",
    "    window_overlap: float = 0.75,\n",
    "    min_freq: float = 10000,\n",
    "    max_freq: float = 120000\n",
    "):\n",
    "    recording = Recording.from_file(path, compute_hash=False)\n",
    "    wave = audio.load_clip(\n",
    "        Clip(\n",
    "            recording=recording,\n",
    "            start_time=max((start_time+end_time)/2 - time_buffer, 0),\n",
    "            end_time=min((start_time+end_time)/2  + time_buffer, recording.duration),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    spectrogram = audio.compute_spectrogram(\n",
    "        wave,\n",
    "        window_size=window_size,\n",
    "        hop_size=window_size * (1 - window_overlap),\n",
    "    )\n",
    "\n",
    "    db_spectrogram = arrays.to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    return db_spectrogram.sel(\n",
    "        time=slice((start_time+end_time)/2 - time_buffer, (start_time+end_time)/2 + time_buffer),\n",
    "        frequency=slice(10000, 100000),\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de357f96-1c35-4667-b8ae-7b70d9595fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#Plotting many representative spectrograms - zoomed\n",
    "######\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Cleaned-up version of pca_df_2\n",
    "pca_df_2_clean = pca_df_2.dropna(subset=['files'])\n",
    "\n",
    "def plot_cluster_spectrograms(pca_df_2_clean, cluster_label, n_samples=100):\n",
    "    \n",
    "    # Filter for the specified cluster\n",
    "    cluster_subset = pca_df_2_clean[pca_df_2_clean[\"Cluster_32PCs\"] == cluster_label]\n",
    "    \n",
    "    # Select up to n_samples from the cluster\n",
    "    if len(cluster_subset) > n_samples:\n",
    "        cluster_subset = cluster_subset.sample(n=n_samples, random_state=1)\n",
    "        \n",
    "    # Set up figure for multiple subplots\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(25, 25))  \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, row in enumerate(cluster_subset.itertuples()):\n",
    "        \n",
    "        # Extract the location of the call from each row\n",
    "        file_path = row.files\n",
    "        start_time = row.start_time\n",
    "        end_time = row.end_time\n",
    "        low_freq = row.low_freq\n",
    "        high_freq = row.high_freq\n",
    "        \n",
    "        # Use extract_detection_section\n",
    "        spec = extract_detection_section(file_path, start_time, end_time, low_freq, high_freq)\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        ax = axes[i]\n",
    "        spec.plot(cmap=\"magma\", add_colorbar=False, add_labels=False, ax=ax)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f2136-e72e-4fda-91a8-a62f8b50b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "#Plotting many representative spectrograms\n",
    "######\n",
    "\n",
    "def plot_cluster_spectrograms(pca_df_2_clean, cluster_label, n_samples=25, target_length=128):\n",
    "    \n",
    "    def generate_spectrogram(audio, sr, n_fft=512, hop_length=128):\n",
    "        \"\"\"Generate a spectrogram using STFT and convert it to a decibel scale.\"\"\"\n",
    "        S = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "        return S_db\n",
    "    \n",
    "    # Filter for the specified cluster\n",
    "    cluster_subset = pca_df_2_clean[pca_df_2_clean[\"Cluster_32PCs\"] == cluster_label]\n",
    "    \n",
    "    # Select up to n_samples from the cluster\n",
    "    if len(cluster_subset) > n_samples:\n",
    "        cluster_subset = cluster_subset.sample(n=n_samples, random_state=42)\n",
    "        \n",
    "    # Set up figure for multiple subplots\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(15, 12))  \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, row in enumerate(cluster_subset.itertuples()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Extract the location of the call from each row\n",
    "        file_path = row.files\n",
    "        start_time = row.start_time\n",
    "        end_time = row.end_time\n",
    "        \n",
    "        # Load audio and isolate the call (with ±0.1s buffer)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        start_sample = int(((start_time+end_time)/2 - 0.01) * sr)\n",
    "        end_sample = int(((start_time+end_time)/2 + 0.01) * sr)\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "        \n",
    "        # Generate spectrogram\n",
    "        spec = generate_spectrogram(audio_segment, sr)\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        ax = axes[i]\n",
    "        img = librosa.display.specshow(\n",
    "            spec,\n",
    "            sr=sr,\n",
    "            hop_length=256,\n",
    "            cmap=\"magma\",\n",
    "            y_axis=\"hz\",\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        \n",
    "    # Fill remaining subplot spaces if there are fewer than 25 samples\n",
    "    for i in range(len(cluster_subset), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.suptitle(f\"Representative Spectrograms for Cluster {cluster_label}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe81307-f7d3-49d9-a28a-48e8c5bb4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_spectrograms(pca_df_2_clean, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6060e-b339-4c40-9ded-3331cc29e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#Plotting the average of every spectrogram\n",
    "#####################################################\n",
    "\n",
    "#come back to this\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "\n",
    "def plot_average_spectrogram(pca_df_2_clean, cluster_label, n_samples=None, target_length=128):\n",
    "\n",
    "    def generate_spectrogram(audio, sr, n_fft=512, hop_length=128):\n",
    "        \"\"\"Generate a spectrogram using STFT and convert it to a decibel scale.\"\"\"\n",
    "        S = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "        return S_db\n",
    "\n",
    "    # Filter for the specified cluster\n",
    "    cluster_subset = pca_df_2_clean[pca_df_2_clean[\"Cluster_32PCs\"] == cluster_label]\n",
    "    \n",
    "    # Select samples from the cluster (if n_samples specified)\n",
    "    if n_samples is not None and len(cluster_subset) > n_samples:\n",
    "        cluster_subset = cluster_subset.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    # List to store resampled spectrograms\n",
    "    spectrograms = []\n",
    "    \n",
    "    # Process each audio file\n",
    "    for row in cluster_subset.itertuples():\n",
    "        # Load and process audio\n",
    "        wav_path = row.files\n",
    "        audio, sr = librosa.load(wav_path, sr=None)\n",
    "        \n",
    "        # Isolate the call using start and end times\n",
    "        start_sample = int((row.start_time - 0.1) * sr)\n",
    "        end_sample = int((row.end_time + 0.1) * sr)\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "        \n",
    "        # Generate spectrogram\n",
    "        spec = generate_spectrogram(audio_segment, sr)\n",
    "        \n",
    "        # Resample each frequency bin to target length\n",
    "        resampled_spec = np.zeros((spec.shape[0], target_length))\n",
    "        for freq_idx in range(spec.shape[0]):\n",
    "            resampled_spec[freq_idx, :] = signal.resample(spec[freq_idx, :], target_length)\n",
    "    \n",
    "        spectrograms.append(resampled_spec)\n",
    "    \n",
    "    # Convert to numpy array and calculate mean\n",
    "    spectrograms = np.array(spectrograms)\n",
    "    average_spec = np.mean(spectrograms, axis=0)\n",
    "\n",
    "    # Plot the average spectrogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    librosa.display.specshow(\n",
    "        average_spec,\n",
    "        cmap=\"magma\",\n",
    "        hop_length=256,\n",
    "        sr=sr,\n",
    "        y_axis=\"hz\"\n",
    "    )\n",
    "    plt.colorbar(format=\"%+2.f dB\")\n",
    "    plt.title(f\"Average Spectrogram for Cluster {cluster_label}\\n(n={len(spectrograms)} samples)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return average_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34104e-5ad5-4743-8c4a-dcfb4c36072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_spectrogram(pca_df_2_clean, 6, n_samples=None, target_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822b2c1-8375-4513-8773-9f5ad0516363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Statistics#\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Calculate summary statistics grouped by Cluster_32PCs\n",
    "summary_stats = (\n",
    "    pca_df_2.assign(duration=pca_df_2[\"end_time\"] - pca_df_2[\"start_time\"])\n",
    "    .groupby(\"Cluster_32PCs\")[[\"high_freq\", \"low_freq\", \"duration\", \"bandwidth\", \"gradient\"]]\n",
    "    .describe()\n",
    ")\n",
    "\n",
    "summary_stats_transposed = summary_stats.T  # Transpose to have stats as rows\n",
    "\n",
    "summary_stats_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a47941-5064-416e-b4c4-4e650c0e4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def plot_cluster_normal_distribution(pca_df_2, cluster_label, threshold=2):\n",
    "    # Filter for the specified cluster\n",
    "    cluster_subset = pca_df_2[pca_df_2[\"Cluster_32PCs\"] == cluster_label]\n",
    "    \n",
    "    # Plot normal distribution for high_freq, low_freq, and duration\n",
    "    features = ['high_freq', 'low_freq', 'duration', 'bandwidth', 'gradient']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 5))\n",
    "    for i, feature in enumerate(features):\n",
    "        sns.histplot(cluster_subset[feature], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f\"{feature} Distribution for Cluster {cluster_label}\")\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e130db0-2f01-4ec0-ba2c-99dab508be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Plot for cluster 0\n",
    "plot_cluster_normal_distribution(pca_df_2, cluster_label=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c01acf-d22d-4bb0-a043-c209bd088acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df_2, x='low_freq', y='high_freq', hue='Cluster_32PCs', palette='Set1', s=20)\n",
    "\n",
    "# Add labels and a title\n",
    "plt.title('Scatter plot of low_freq vs high_freq')\n",
    "plt.xlabel('Low Frequency')\n",
    "plt.ylabel('High Frequency')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0908a86-37a6-4c1b-8138-c9894e152861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df_2, x='low_freq', y='duration', hue='Cluster_32PCs', palette='Set1', s=20)\n",
    "\n",
    "# Add labels and a title\n",
    "plt.title('Scatter plot of low_freq vs duration')\n",
    "plt.xlabel('Low Frequency')\n",
    "plt.ylabel('Duration')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd498a3b-8882-44d5-b299-ee0846d304d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df_2, x='high_freq', y='duration', hue='Cluster_32PCs', palette='Set1', s=20)\n",
    "\n",
    "# Add labels and a title\n",
    "plt.title('Scatter plot of high_freq vs duration')\n",
    "plt.xlabel('High Frequency')\n",
    "plt.ylabel('Duration')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f85e5-9717-42d6-9dab-cc14bcf166f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#Assign Clusters to Guilds\n",
    "#####################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686c468-ea5a-4182-baf6-1f526890beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc17926-fb2f-41de-8629-db68846d786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_groups(row):\n",
    "    cluster = row['Cluster_32PCs']\n",
    "    duration = row['duration']\n",
    "    bandwidth = row['bandwidth']\n",
    "    gradient = row['gradient']\n",
    "    \n",
    "    if cluster == 0:\n",
    "        return '0'\n",
    "    elif cluster == 1:\n",
    "        return '1.2' if duration > 0.03 else '1.1'\n",
    "    elif cluster == 2:\n",
    "        return '2'\n",
    "    elif cluster == 3:\n",
    "        return '3'\n",
    "    elif cluster == 4:\n",
    "        return '4'\n",
    "    elif cluster == 5:\n",
    "        return '5'\n",
    "    elif cluster == 6:\n",
    "      return '6.1' if gradient < 2500000 else '6.2'\n",
    "\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add the new column\n",
    "pca_df_2['group'] = pca_df_2.apply(assign_groups, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634e1e6-9046-4e7b-ab94-db79250f1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_guild(row):\n",
    "    cluster = row['Cluster_32PCs']\n",
    "    duration = row['duration']\n",
    "    bandwidth = row['bandwidth']\n",
    "    gradient = row['gradient']\n",
    "    \n",
    "    if cluster == 0:\n",
    "        return 'Edge'\n",
    "    elif cluster == 1:\n",
    "        return 'Clutter' if duration > 0.03 else 'Edge'\n",
    "    elif cluster == 2:\n",
    "        return 'Clutter'\n",
    "    elif cluster == 3:\n",
    "        return 'Edge'\n",
    "    elif cluster == 4:\n",
    "        return 'Clutter'\n",
    "    elif cluster == 5:\n",
    "        return 'Open'\n",
    "    elif cluster == 6:\n",
    "        return 'Edge' if gradient < 2500000 else 'Clutter'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Add the new column\n",
    "pca_df_2['guild'] = pca_df_2.apply(assign_guild, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413cb97-c63c-4c5c-9c30-fd78dda923b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427ee0d-dcde-4a70-8395-3f5f80146e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed of sound in air in m/s\n",
    "speed_of_sound = 343  \n",
    "\n",
    "# Define a function to get an example spectrogram for a cluster (placeholder)\n",
    "def get_example_spectrogram(cluster):\n",
    "    return f\"spectrogram_{cluster}.png\"  # Replace with real logic or paths later\n",
    "\n",
    "# Create the summary table\n",
    "cluster_summary = (\n",
    "    pca_df_2\n",
    "    .groupby('group')\n",
    "    .apply(lambda df: pd.Series({\n",
    "        'Number of Calls': len(df),\n",
    "        'Frequency Range (Hz)': f\"{df['low_freq'].min():.0f} - {df['high_freq'].max():.0f}\",\n",
    "        'Mean Frequency (Hz)': ((df['low_freq'] + df['high_freq']) / 2).mean(),\n",
    "        'Mean Bandwidth (Hz)': df['bandwidth'].mean(),\n",
    "        'Mean Duration (s)': df['duration'].mean(),\n",
    "        'Wavelength Range (m)': f\"{speed_of_sound / df['high_freq'].max():.3f} - {speed_of_sound / df['low_freq'].min():.3f}\",\n",
    "        'Mean Wavelength (m)': speed_of_sound / ((df['high_freq'].mean() + df['low_freq'].mean()) / 2),\n",
    "        'Guild': df['guild'].mode()[0] if not df['guild'].mode().empty else 'Unknown',\n",
    "        'Example Spectrogram': get_example_spectrogram(df.name),\n",
    "        'Suggested Species': ''  # Fill manually later\n",
    "    }))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8338d-f954-4a72-8ddd-1f58f6c8f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b72668-6074-4b7d-b656-6130b476914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary.to_csv(\"cluster_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad30bbe-ba70-41c7-8f10-2ac0259d68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "\n",
    "\n",
    "        #Occupancy Modelling#\n",
    "\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c400a2-5f1d-446b-ac4a-d40f5f58b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to make a new dataframe for all the old files#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152b1cb-63ea-4c45-a06e-2f0ef88a2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file and extract only the 'path' column\n",
    "csv_file_path = 'kenya_audio_2019/batdetect2/metadata/merged.csv'\n",
    "files_df = pd.read_csv(csv_file_path, usecols=['path'])\n",
    "\n",
    "# Rename the 'path' column to 'file_path'\n",
    "files_df.rename(columns={'path': 'file_path'}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(files_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0242eb9-03a2-4a15-92ce-d3298f7c2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to give all files that had an output a presence score = 1\n",
    "#Then all files that didn't have an output an absence score = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc80d4-8736-466e-8150-8be2e2058866",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df.head()\n",
    "files_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fc3c6-93d6-4582-9b9e-1d645a15298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db259a-4af9-4059-9c5e-6701795df005",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab8368-c260-4a2b-97c6-b6759f022df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2.to_csv('pca_df_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9bde6-7cb7-40d6-8f76-f3616dccabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = embeddings_df.merge(\n",
    "    pca_df_2[['file_path', 'Cluster_32PCs', 'guild']].drop_duplicates(subset=['file_path']), \n",
    "    on='file_path', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ac154-70fc-4a75-9661-04bda101e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b01ef7-b5de-4d01-88bd-2fb5f9a5957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29134b-29bf-429c-bf7f-987b32065cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_df = embeddings_df.drop(embeddings_df.columns[2:42], axis=1)\n",
    "occupancy_df = occupancy_df.drop(embeddings_df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde8a07-287f-48bd-a853-eba86354d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039da5e8-d0ea-4f42-87e3-3d4486e4cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eae7a5-43db-4cd7-aedd-8e22017e1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving this for Activity GMM as Jenny Linden Did#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ceab3-991f-45dc-a46c-5f58a4b527d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_df.to_csv(\"activity_df_60s.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da8ff6-6635-4e37-8212-871940f8c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay probelm of multiple recordings have different Clustered groups in so need to be sure to make unique_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8da1c-cd13-471d-8ff4-7a1e56dec881",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filenames = occupancy_df.loc[\n",
    "    occupancy_df[['file_path', 'Cluster_32PCs']].drop_duplicates().index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa1028-c623-4f5a-a79d-7bdde7cef1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filenames.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9108550-89df-4efd-8874-54414d24168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "# Group by 'file_name' and count unique clusters\n",
    "unique_clusters_per_file = occupancy_df.groupby('file_path')['Cluster_32PCs'].nunique().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "unique_clusters_per_file.columns = ['file_name', 'unique_cluster_count']\n",
    "\n",
    "# Display the result\n",
    "print(unique_clusters_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245548e-35a1-4b24-a3d2-72331afe73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each file_name\n",
    "file_name_counts = occupancy_df['file_path'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "file_name_counts.columns = ['file_name', 'occurrence_count']\n",
    "\n",
    "# Display the result\n",
    "print(file_name_counts)\n",
    "\n",
    "sorted_file_name_counts = file_name_counts.sort_values(by='occurrence_count', ascending=False)\n",
    "\n",
    "# Save the sorted DataFrame to a CSV file without the index\n",
    "sorted_file_name_counts.to_csv('file_name_counts_sorted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a688a53-b2ef-457f-befe-f2b6a7805c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f13ba-de33-40ee-8aeb-517eb60bb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RENAME UNIQUE_FILENAMES --> OCCUPANCY_DF OR SOMETHING ALONG THOSE LINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5adb14-6a20-4c99-aa33-2386368e38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filenames['Presence'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f6ebf-03bf-4fa4-81b3-bd2dcc50f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filenames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8632e7-cb18-4660-b056-55952b79444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I want to merge outputs df and files_df by filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51851488-631f-483d-8433-94d9b4607f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files_df with outputs_df on \"filename\", keeping all rows from files_df\n",
    "merged_df = files_df.merge(unique_filenames, on=\"file_path\", how=\"left\")\n",
    "\n",
    "# Fill NaN values in 'presence' with 0 (indicating absence)\n",
    "merged_df[\"Presence\"] = merged_df[\"Presence\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f3fbb-c293-4d89-8abf-b185e3744b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2b404-f060-4047-be10-661a588205ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['site'] = merged_df['file_path'].apply(extract_site) \n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a798b-1089-4a36-8cf8-e2ca3f441acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['date'] = merged_df['file_path'].apply(extract_date) \n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c97e01-fc35-4a5f-99c6-f9e3e0188c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['time'] = merged_df['file_path'].apply(extract_time) \n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e00bd-fd0f-4f5e-b4c2-f6213da0dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop('duration', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1ca57-3db2-41f3-82dd-133cacace6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cbc81-f05b-40d0-897c-2fcea44613d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"merged_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3afe0-cf78-41b5-9fd6-4d5cb63a35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW I HAVE MERGED_DF WHICH HAS PRESENCE(1) ABSENCE(0) FOR EACH FILE NEED TO ADD DATE AND TIME AND SITE FOR NON DETECTIONS TOO#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659aa18-b950-4d08-a57a-a3e2e9a3a0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d958b-a6ab-469f-90b5-818004c47e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tom_Mara",
   "language": "python",
   "name": "tom_mara"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
